https://github.com/kubernetes/kubeadm/issues/546

Take the option1 to install etcd in 3 work nodes and keep 2 masters in cluster.
Except mentioned specifically, all steps runs on all etcd 3 work nodes.

- [Install cfssl - CLI & API tool for TLS/SSL](#install-cfssl---cli-api-tool-for-tlsssl)
- [For etcd0 only](#for-etcd0-only)
- [Create SSH access for all etcd work ondes](#create-ssh-access-for-all-etcd-work-ondes)
- [Generate etcd server and peer certs](#generate-etcd-server-and-peer-certs)
- [Run etcd with systemd](#run-etcd-with-systemd)
- [Troubleshoot etcd](#troubleshoot-etcd)
- [Set up master Load Balancer](#set-up-master-load-balancer)

# Install cfssl - CLI & API tool for TLS/SSL
```
curl -o /usr/local/bin/cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
curl -o /usr/local/bin/cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
chmod +x /usr/local/bin/cfssl*
```

# For etcd0 only
```
mkdir -p /etc/kubernetes/pki/etcd
cd /etc/kubernetes/pki/etcd

cat >ca-config.json <<EOL
 {
     "signing": {
         "default": {
             "expiry": "43800h"
         },
         "profiles": {
             "server": {
                 "expiry": "43800h",
                 "usages": [
                     "signing",
                     "key encipherment",
                     "server auth",
                     "client auth"
                 ]
             },
             "client": {
                 "expiry": "43800h",
                 "usages": [
                     "signing",
                     "key encipherment",
                     "client auth"
                 ]
             },
             "peer": {
                 "expiry": "43800h",
                 "usages": [
                     "signing",
                     "key encipherment",
                     "server auth",
                     "client auth"
                 ]
             }
         }
     }
 }
 EOL


cat >ca-csr.json <<EOL
{
  "CN": "etcd",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "San Francisco",
      "O": "Example Company",
      "OU": "Operations",
      "ST": "California"
    }
  ]
}
EOL


# generate the CA certs like so:
cfssl gencert -initca ca-csr.json | cfssljson -bare ca -

# generate etcd clients certs
cat >client.json <<EOL
 {
     "CN": "client",
     "key": {
         "algo": "ecdsa",
         "size": 256
     }
 }
 EOL

cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=client client.json | cfssljson -bare client
```
This should result in `client.pem` and `client-key.pem` being created.

# Create SSH access for all etcd work ondes
```
export PEER_NAME=$(hostname)
export PRIVATE_IP=$(ip addr show eth1 | grep -Po 'inet \K[\d.]+')
# export PRIVATE_IP=$(ip addr show enp0s3 | grep -Po 'inet \K[\d.]+')
```
Make sure that `eth1` corresponds to the network interface for the IPv4 address of the private network. This might vary depending on your networking setup, so please check by running `echo $PRIVATE_IP` before continuing. Or simply replace `$(ip addr show eth1 | grep -Po 'inet \K[\d.]+')` with IPv4.

```
ssh-keygen -t rsa -b 4096 -C "<email>"
```
Replace <email> with your email, a placeholder, or an empty string. Keep hitting enter until files exist in ~/.ssh.

Output the contents of the public key file for etcd1 and etcd2, like so:
```
cat ~/.ssh/id_rsa.pub
```

Finally, copy the output for each and paste them into etcd0’s `~/.ssh/authorized_keys` file. This will permit etcd1 and etcd2 to SSH in to the machine etcd0.


# Generate etcd server and peer certs
In order to generate certs, each etcd machine needs the root CA generated by etcd0. On etcd1 and etcd2, run the following:
```
mkdir -p /etc/kubernetes/pki/etcd
cd /etc/kubernetes/pki/etcd

# first time ssh connection to etcd0 needs a confirm 'yes'
scp root@192.168.0.50:/etc/kubernetes/pki/etcd/ca.pem .

scp root@192.168.0.50:/etc/kubernetes/pki/etcd/ca-key.pem .
scp root@192.168.0.50:/etc/kubernetes/pki/etcd/client.pem .
scp root@192.168.0.50:/etc/kubernetes/pki/etcd/client-key.pem .
scp root@192.168.0.50:/etc/kubernetes/pki/etcd/ca-config.json .
```
Where `192.168.0.45` corresponds to the public or private IPv4 of etcd0.
`scp` secure copy(remote file copy program)

Once this is done, run the following on all etcd machines:
```
cd /etc/kubernetes/pki/etcd

cfssl print-defaults csr > config.json
sed -i '0,/CN/{s/example\.net/'"$PEER_NAME"'/}' config.json
sed -i 's/www\.example\.net/'"$PRIVATE_IP"'/' config.json
sed -i 's/example\.net/'"$PUBLIC_IP"'/' config.json

cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server config.json | cfssljson -bare server
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=peer config.json | cfssljson -bare peer
```
This will result in the following files: `peer.pem`, `peer-key.pem`, `server.pem`, `server-key.pem`.

# Run etcd with systemd
Install etcd binaries like so
```
export ETCD_VERSION=v3.1.10
curl -sSL https://github.com/coreos/etcd/releases/download/${ETCD_VERSION}/etcd-${ETCD_VERSION}-linux-amd64.tar.gz | tar -xzv --strip-components=1 -C /usr/local/bin/
rm -rf etcd-$ETCD_VERSION-linux-amd64*
```

Generate the environment file that systemd will use:
```
touch /etc/etcd.env
echo "PEER_NAME=$PEER_NAME" >> /etc/etcd.env
echo "PRIVATE_IP=$PRIVATE_IP" >> /etc/etcd.env
```

* Copy the systemd unit, make sure you replace with correct IPv4 address <etcd0-ip-address>192.168.0.50, <etcd1-ip-address>192.168.0.51 and <etcd2-ip-address>192.168.0.52 for file /etc/systemd/system/etcd.service.
* Change `https` to `http` for below file if you are using private IPv4 locally.
* Update /etc/resolv.conf file with IPs to bypass DNS
```
cat >/etc/systemd/system/etcd.service <<EOL
[Unit]
Description=etcd
Documentation=https://github.com/coreos/etcd
Conflicts=etcd.service
Conflicts=etcd2.service

[Service]
EnvironmentFile=/etc/etcd.env
Type=notify
Restart=always
RestartSec=5s
LimitNOFILE=40000
TimeoutStartSec=0

ExecStart=/usr/local/bin/etcd --name ${PEER_NAME} \
    --data-dir /var/lib/etcd \
    --listen-client-urls http://${PRIVATE_IP}:2379 \
    --advertise-client-urls http://${PRIVATE_IP}:2379 \
    --listen-peer-urls http://${PRIVATE_IP}:2380 \
    --initial-advertise-peer-urls http://${PRIVATE_IP}:2380 \
    --cert-file=/etc/kubernetes/pki/etcd/server.pem \
    --key-file=/etc/kubernetes/pki/etcd/server-key.pem \
    --client-cert-auth \
    --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.pem \
    --peer-cert-file=/etc/kubernetes/pki/etcd/peer.pem \
    --peer-key-file=/etc/kubernetes/pki/etcd/peer-key.pem \
    --peer-client-cert-auth \
    --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.pem \
    --initial-cluster etcd0=http://192.168.0.50:2380,etcd1=http://192.168.0.51:2380,etcd2=http://192.168.0.52:2380 \
    --initial-cluster-token my-etcd-token \
    --initial-cluster-state new

[Install]
WantedBy=multi-user.target
EOL
```
Trick for DNS if you run locally IPv4 but restart will erase it.
```
nano /etc/resolv.conf

# add below everytime after restart
etcd0 192.168.0.50
etcd1 192.168.0.51
etcd2 192.168.0.52
```
Finally, launch etcd like so:
```
systemctl daemon-reload
systemctl start etcd

systemctl status etcd
```

# Troubleshoot etcd
Debug service start
```
systemctl status etcd

systemctl --no-page -t service -a | grep etcd
journalctl -ex

journalctl --no-page -u etcd.service

etcdctl member list
etcdctl cluster-health
```

If error of already member, stop service and delete file then restart.</br>
`systemctl start etcd` can't start the etcd because node4 is not in cluster
and it's already member...
Delete `rm -rf /var/lib/etcd` as you specify in config as `--date-dir`

http://blog.csdn.net/god_wot/article/details/77854093

If error of no such file, check before if you miss generate any files.
```
Feb 07 22:59:50 etcd0 etcd[1779]: peerTLS: cert = /etc/kubernetes/pki/etcd/peer.pem, key = /etc/kubernetes/pki/etcd/peer-key.pem, ca = , trusted-ca = /etc/kubernetes/pki/etcd/ca.pem, client-cert-auth = true
Feb 07 22:59:50 etcd0 etcd[1779]: open /etc/kubernetes/pki/etcd/peer.pem: no such file or directory
Feb 07 22:59:50 etcd0 systemd[1]: etcd.service: Main process exited, code=exited, status=1/FAILURE
Feb 07 22:59:50 etcd0 systemd[1]: Failed to start etcd.
Feb 07 22:59:50 etcd0 systemd[1]: etcd.service: Unit entered failed state.
Feb 07 22:59:50 etcd0 systemd[1]: etcd.service: Failed with result 'exit-code'.
```

If `systemctl status etcd` is active but `etcdctl cluster-health` is showing
cluster unhealth, because `etcdctl` default check localhost:2379, but we have configured differently, so we pass --endpoints
```
root@etcd0:~# etcdctl member list
Error:  client: etcd cluster is unavailable or misconfigured; error #0: dial tcp 127.0.0.1:4001: getsockopt: connection refused
; error #1: dial tcp 127.0.0.1:2379: getsockopt: connection refused

error #0: dial tcp 127.0.0.1:4001: getsockopt: connection refused
error #1: dial tcp 127.0.0.1:2379: getsockopt: connection refused

root@etcd0:~# etcdctl member list
Error:  client: etcd cluster is unavailable or misconfigured; error #0: dial tcp 127.0.0.1:4001: getsockopt: connection refused
; error #1: dial tcp 127.0.0.1:2379: getsockopt: connection refused

error #0: dial tcp 127.0.0.1:4001: getsockopt: connection refused
error #1: dial tcp 127.0.0.1:2379: getsockopt: connection refused


root@etcd0:~# etcdctl --endpoints 'http://192.168.0.50:2379' cluster-health
member 79daeb26f9651b44 is healthy: got healthy result from http://192.168.0.50:2379
member c52ff0ed834fd076 is healthy: got healthy result from http://192.168.0.51:2379
member de245bc5339308a0 is healthy: got healthy result from http://192.168.0.52:2379
cluster is healthy
```

Make sure the service is started with know `PEER_NAME` and `PRIVATE_IP`
```
echo $PEER_NAME
echo $PRIVATE_IP
```
Otherwise add them into ~/.profile
```
PEER_NAME=etcd0
PRIVATE_IP=192.168.0.50
```

Successfully created the etcd cluster, you see the status is active(running) on any etcd node
```
root@etcd2:/etc/kubernetes/pki/etcd# systemctl status etcd
● etcd.service - etcd
   Loaded: loaded (/etc/systemd/system/etcd.service; disabled; vendor preset: enabled)
   Active: active (running) since Wed 2018-02-07 23:42:58 CET; 6s ago
     Docs: https://github.com/coreos/etcd
 Main PID: 2760 (etcd)
    Tasks: 7 (limit: 4915)
   CGroup: /system.slice/etcd.service
           └─2760 /usr/local/bin/etcd --name etcd2 --data-dir /var/lib/etcd --listen-client-urls http://192.168.0.52:2379 --advertise-client-urls http://192.168.0.52:2379 --listen-peer-urls http://192.168.0.52:2380 --initial-advertise-peer-urls http://192.168.0.52:2380 --cert-file=/etc/kubernetes/pki/etcd/server.pem --key-file=/etc/kubernetes/pki/etcd/server-key.pem --client-cert-auth --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.pem --peer-cert-file=/etc/kubernetes/pki/etcd/peer.pem --peer-key-file=/etc/kubernetes/pki/etcd/peer-key.pem --peer-client-cert-auth --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.pem --initial-cluster etcd0=http://192.168.0.50:2380,etcd1=http://192.168.0.51:2380,etcd2=http://192.168.0.52:2380 --initial-cluster-token my-etcd-token --initial-cluster-state new
```

If after VMs reboot, etcd cluster and service failed to restart, do below
on etcd0 first(will pending if etcd1 & etcd 2 not join) then etcd1 and etcd2.
```
systemctl stop etcd
rm -rf /var/lib/etcd/
systemctl daemon-reload
systemctl start etcd
```

# Set up master Load Balancer
https://www.digitalocean.com/community/tutorials/how-to-set-up-nginx-load-balancing

https://www.digitalocean.com/community/tutorials/how-to-set-up-highly-available-haproxy-servers-with-keepalived-and-floating-ips-on-ubuntu-14-04

Nginx runs on a seperate machine
https://www.upcloud.com/support/how-to-set-up-load-balancing/

Nginx official round-robin, least-conn algo:
https://www.nginx.com/resources/admin-guide/load-balancer/

Install nginx in a new VM, after installation, open a browser and check
`localhost`, you should see the nginx page.
```
apt-get install nginx
```

Config `/etc/nginx/sites-available/default`
```
upstream www {
        server 192.168.0.50;
        server 192.168.0.51;
        server 192.168.0.52;
}
```

Generate SSH keys for each of the master nodes by `ssh-keygen -t rsa -b 4096 -C "<email>"`. After doing this, each master will have an SSH key in `~/.ssh/id_rsa.pub` and an entry in etcd0’s `~/.ssh/authorized_keys` file.

Run the following to get cert from etcd0 to master:
```
mkdir -p /etc/kubernetes/pki/etcd
scp root@<etcd0-ip-address>:/etc/kubernetes/pki/etcd/ca.pem /etc/kubernetes/pki/etcd
scp root@<etcd0-ip-address>:/etc/kubernetes/pki/etcd/client.pem /etc/kubernetes/pki/etcd
scp root@<etcd0-ip-address>:/etc/kubernetes/pki/etcd/client-key.pem /etc/kubernetes/pki/etcd
```

Run `kubeadm init --config=config.yaml`
```
cat >config.yaml <<EOF
apiVersion: kubeadm.k8s.io/v1alpha1
kind: MasterConfiguration
api:
  advertiseAddress: <private-ip>
etcd:
  endpoints:
  - https://<etcd0-ip-address>:2379
  - https://<etcd1-ip-address>:2379
  - https://<etcd2-ip-address>:2379
  caFile: /etc/kubernetes/pki/etcd/ca.pem
  certFile: /etc/kubernetes/pki/etcd/client.pem
  keyFile: /etc/kubernetes/pki/etcd/client-key.pem
networking:
  podSubnet: <podCIDR>
apiServerCertSANs:
- <load-balancer-ip>
apiServerExtraArgs:
  endpoint-reconciler-type=lease
EOF
```

Ensure that the following placeholders are replaced:

`<private-ip>` with the private IPv4 of the master server.
`<etcd0-ip>`, `<etcd1-ip>` and `<etcd2-ip>` with the IP addresses of your three etcd nodes
`<podCIDR>` with your Pod CIDR (Flannel CNI `--pod-network-cidr=10.244.0.0/16`). Please read the CNI network section of the docs for more information. Some CNI providers do not require a value to be set.
`<load-balancer-ip>` from nginx
```
apiVersion: kubeadm.k8s.io/v1alpha1
kind: MasterConfiguration
api:
  advertiseAddress: 192.168.0.43
etcd:
  endpoints:
  - http://192.168.0.50:2379
  - http://192.168.0.51:2379
  - http://192.168.0.52:2379
  caFile: /etc/kubernetes/pki/etcd/ca.pem
  certFile: /etc/kubernetes/pki/etcd/client.pem
  keyFile: /etc/kubernetes/pki/etcd/client-key.pem
networking:
  podSubnet: 10.244.0.0/16
apiServerCertSANs:
- 192.168.0.53
apiServerExtraArgs:
  endpoint-reconciler-type=lease
```