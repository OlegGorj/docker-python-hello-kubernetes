https://github.com/kubernetes/kubeadm/issues/546

Take the option1 Hosting etcd cluster on separate compute nodes (Virtual Machines) and extra 3 masters in k8s cluster.

Except mentioned specifically, all steps runs on all etcd 3 work nodes.


- [Install cfssl - CLI & API tool for TLS/SSL](#install-cfssl---cli-api-tool-for-tlsssl)
- [For etcd0 only](#for-etcd0-only)
- [Create SSH access for all etcd work ondes](#create-ssh-access-for-all-etcd-work-ondes)
- [Generate etcd server and peer certs](#generate-etcd-server-and-peer-certs)
- [Run etcd with systemd](#run-etcd-with-systemd)
- [Troubleshoot etcd](#troubleshoot-etcd)
- [Set up master Load Balancer](#set-up-master-load-balancer)
    - [Exploring the nginx.conf File](#exploring-the-nginxconf-file)
    - [Exploring the Default Server Block](#exploring-the-default-server-block)
    - [Config ngxin on Master0](#config-ngxin-on-master0)
- [VirtualIP failover with Keepalived](#virtualip-failover-with-keepalived)
    - [Troubleshoot kubeadm init issues](#troubleshoot-kubeadm-init-issues)
- [Run kubeadm init on other masters: master1 and master2](#run-kubeadm-init-on-other-masters-master1-and-master2)
- [Add worksers in k8s cluster](#add-worksers-in-k8s-cluster)

# Install cfssl - CLI & API tool for TLS/SSL
```
curl -o /usr/local/bin/cfssl https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
curl -o /usr/local/bin/cfssljson https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
chmod +x /usr/local/bin/cfssl*
```

# For etcd0 only
```
mkdir -p /etc/kubernetes/pki/etcd
cd /etc/kubernetes/pki/etcd

cat >ca-config.json <<EOL
 {
     "signing": {
         "default": {
             "expiry": "43800h"
         },
         "profiles": {
             "server": {
                 "expiry": "43800h",
                 "usages": [
                     "signing",
                     "key encipherment",
                     "server auth",
                     "client auth"
                 ]
             },
             "client": {
                 "expiry": "43800h",
                 "usages": [
                     "signing",
                     "key encipherment",
                     "client auth"
                 ]
             },
             "peer": {
                 "expiry": "43800h",
                 "usages": [
                     "signing",
                     "key encipherment",
                     "server auth",
                     "client auth"
                 ]
             }
         }
     }
 }
 EOL


cat >ca-csr.json <<EOL
{
  "CN": "etcd",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
    {
      "C": "US",
      "L": "San Francisco",
      "O": "Example Company",
      "OU": "Operations",
      "ST": "California"
    }
  ]
}
EOL


# generate the CA certs like so:
cfssl gencert -initca ca-csr.json | cfssljson -bare ca -

# generate etcd clients certs
cat >client.json <<EOL
 {
     "CN": "client",
     "key": {
         "algo": "ecdsa",
         "size": 256
     }
 }
 EOL

cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=client client.json | cfssljson -bare client
```
This should result in `client.pem` and `client-key.pem` being created.

# Create SSH access for all etcd work ondes
Save below command in `~/.profile` file:
```
export PEER_NAME=$(hostname)
export PRIVATE_IP=$(ip addr show eth1 | grep -Po 'inet \K[\d.]+')
# export PRIVATE_IP=$(ip addr show enp0s3 | grep -Po 'inet \K[\d.]+')
```
Make sure that `eth1` corresponds to the network interface for the IPv4 address of the private network. This might vary depending on your networking setup, so please check by running `echo $PRIVATE_IP` before continuing. Or simply replace `$(ip addr show eth1 | grep -Po 'inet \K[\d.]+')` with IPv4.

```
ssh-keygen -t rsa -b 4096 -C "<email>"
```
Replace <email> with your email, a placeholder, or an empty string. Keep hitting enter until files exist in ~/.ssh.

Output the contents of the public key file for etcd1 and etcd2, like so:
```
cat ~/.ssh/id_rsa.pub
```

Finally, copy the output for each and paste them into etcd0’s `~/.ssh/authorized_keys` file. This will permit etcd1 and etcd2 to SSH in to the machine etcd0.


# Generate etcd server and peer certs
In order to generate certs, each etcd machine needs the root CA generated by etcd0. On etcd1 and etcd2, run the following:
```
mkdir -p /etc/kubernetes/pki/etcd
cd /etc/kubernetes/pki/etcd

# first time ssh connection to etcd0 needs a confirm 'yes'
scp root@192.168.0.50:/etc/kubernetes/pki/etcd/ca.pem .

scp root@192.168.0.50:/etc/kubernetes/pki/etcd/ca-key.pem .
scp root@192.168.0.50:/etc/kubernetes/pki/etcd/client.pem .
scp root@192.168.0.50:/etc/kubernetes/pki/etcd/client-key.pem .
scp root@192.168.0.50:/etc/kubernetes/pki/etcd/ca-config.json .
```
Where `192.168.0.45` corresponds to the public or private IPv4 of etcd0.
`scp` secure copy(remote file copy program)

Once this is done, run the following on all etcd machines:
```
cd /etc/kubernetes/pki/etcd

cfssl print-defaults csr > config.json
sed -i '0,/CN/{s/example\.net/'"$PEER_NAME"'/}' config.json
sed -i 's/www\.example\.net/'"$PRIVATE_IP"'/' config.json
sed -i 's/example\.net/'"$PUBLIC_IP"'/' config.json

cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=server config.json | cfssljson -bare server
cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=peer config.json | cfssljson -bare peer
```
This will result in the following files: `peer.pem`, `peer-key.pem`, `server.pem`, `server-key.pem`.

# Run etcd with systemd
Install etcd binaries like so
```
export ETCD_VERSION=v3.1.10
curl -sSL https://github.com/coreos/etcd/releases/download/${ETCD_VERSION}/etcd-${ETCD_VERSION}-linux-amd64.tar.gz | tar -xzv --strip-components=1 -C /usr/local/bin/
rm -rf etcd-$ETCD_VERSION-linux-amd64*
```

Generate the environment file that systemd will use:
```
touch /etc/etcd.env
echo "PEER_NAME=$PEER_NAME" >> /etc/etcd.env
echo "PRIVATE_IP=$PRIVATE_IP" >> /etc/etcd.env
```

* Copy the systemd unit, make sure you replace with correct IPv4 address <etcd0-ip-address>192.168.0.50, <etcd1-ip-address>192.168.0.51 and <etcd2-ip-address>192.168.0.52 for file /etc/systemd/system/etcd.service.
* Change `https` to `http` for below file if you are using private IPv4 locally.
* Update /etc/resolv.conf file with IPs to bypass DNS
```
cat >/etc/systemd/system/etcd.service <<EOL
[Unit]
Description=etcd
Documentation=https://github.com/coreos/etcd
Conflicts=etcd.service
Conflicts=etcd2.service

[Service]
EnvironmentFile=/etc/etcd.env
Type=notify
Restart=always
RestartSec=5s
LimitNOFILE=40000
TimeoutStartSec=0

ExecStart=/usr/local/bin/etcd --name ${PEER_NAME} \
    --data-dir /var/lib/etcd \
    --listen-client-urls http://${PRIVATE_IP}:2379 \
    --advertise-client-urls http://${PRIVATE_IP}:2379 \
    --listen-peer-urls http://${PRIVATE_IP}:2380 \
    --initial-advertise-peer-urls http://${PRIVATE_IP}:2380 \
    --cert-file=/etc/kubernetes/pki/etcd/server.pem \
    --key-file=/etc/kubernetes/pki/etcd/server-key.pem \
    --client-cert-auth \
    --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.pem \
    --peer-cert-file=/etc/kubernetes/pki/etcd/peer.pem \
    --peer-key-file=/etc/kubernetes/pki/etcd/peer-key.pem \
    --peer-client-cert-auth \
    --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.pem \
    --initial-cluster etcd0=http://192.168.0.50:2380,etcd1=http://192.168.0.51:2380,etcd2=http://192.168.0.52:2380 \
    --initial-cluster-token my-etcd-token \
    --initial-cluster-state new

[Install]
WantedBy=multi-user.target
EOL
```
Trick for DNS if you run locally IPv4 but restart will erase it.
```
nano /etc/resolv.conf

# add below everytime after restart
etcd0 192.168.0.50
etcd1 192.168.0.51
etcd2 192.168.0.52
```
Finally, launch etcd like so:
```
systemctl daemon-reload
systemctl start etcd

systemctl status etcd
```

# Troubleshoot etcd
Debug service start
```
systemctl status etcd

systemctl --no-page -t service -a | grep etcd
journalctl -ex

journalctl --no-page -u etcd.service

etcdctl member list
etcdctl cluster-health
```

If error of already member, stop service and delete file then restart.</br>
`systemctl start etcd` can't start the etcd because node4 is not in cluster
and it's already member...
Delete `rm -rf /var/lib/etcd` as you specify in config as `--date-dir`

http://blog.csdn.net/god_wot/article/details/77854093

If error of no such file, check before if you miss generate any files.
```
Feb 07 22:59:50 etcd0 etcd[1779]: peerTLS: cert = /etc/kubernetes/pki/etcd/peer.pem, key = /etc/kubernetes/pki/etcd/peer-key.pem, ca = , trusted-ca = /etc/kubernetes/pki/etcd/ca.pem, client-cert-auth = true
Feb 07 22:59:50 etcd0 etcd[1779]: open /etc/kubernetes/pki/etcd/peer.pem: no such file or directory
Feb 07 22:59:50 etcd0 systemd[1]: etcd.service: Main process exited, code=exited, status=1/FAILURE
Feb 07 22:59:50 etcd0 systemd[1]: Failed to start etcd.
Feb 07 22:59:50 etcd0 systemd[1]: etcd.service: Unit entered failed state.
Feb 07 22:59:50 etcd0 systemd[1]: etcd.service: Failed with result 'exit-code'.
```

If `systemctl status etcd` is active but `etcdctl cluster-health` is showing
cluster unhealth, because `etcdctl` default check localhost:2379, but we have configured differently, so we pass --endpoints
```
root@etcd0:~# etcdctl member list
Error:  client: etcd cluster is unavailable or misconfigured; error #0: dial tcp 127.0.0.1:4001: getsockopt: connection refused
; error #1: dial tcp 127.0.0.1:2379: getsockopt: connection refused

error #0: dial tcp 127.0.0.1:4001: getsockopt: connection refused
error #1: dial tcp 127.0.0.1:2379: getsockopt: connection refused

root@etcd0:~# etcdctl member list
Error:  client: etcd cluster is unavailable or misconfigured; error #0: dial tcp 127.0.0.1:4001: getsockopt: connection refused
; error #1: dial tcp 127.0.0.1:2379: getsockopt: connection refused

error #0: dial tcp 127.0.0.1:4001: getsockopt: connection refused
error #1: dial tcp 127.0.0.1:2379: getsockopt: connection refused


root@etcd0:~# etcdctl --endpoints 'http://192.168.0.50:2379' cluster-health
member 79daeb26f9651b44 is healthy: got healthy result from http://192.168.0.50:2379
member c52ff0ed834fd076 is healthy: got healthy result from http://192.168.0.51:2379
member de245bc5339308a0 is healthy: got healthy result from http://192.168.0.52:2379
cluster is healthy
```

Make sure the service is started with know `PEER_NAME` and `PRIVATE_IP`
```
echo $PEER_NAME
echo $PRIVATE_IP
```
Otherwise add them into ~/.profile
```
PEER_NAME=etcd0
PRIVATE_IP=192.168.0.50
```

Successfully created the etcd cluster, you see the status is active(running) on any etcd node
```
root@etcd2:/etc/kubernetes/pki/etcd# systemctl status etcd
● etcd.service - etcd
   Loaded: loaded (/etc/systemd/system/etcd.service; disabled; vendor preset: enabled)
   Active: active (running) since Wed 2018-02-07 23:42:58 CET; 6s ago
     Docs: https://github.com/coreos/etcd
 Main PID: 2760 (etcd)
    Tasks: 7 (limit: 4915)
   CGroup: /system.slice/etcd.service
           └─2760 /usr/local/bin/etcd --name etcd2 --data-dir /var/lib/etcd --listen-client-urls http://192.168.0.52:2379 --advertise-client-urls http://192.168.0.52:2379 --listen-peer-urls http://192.168.0.52:2380 --initial-advertise-peer-urls http://192.168.0.52:2380 --cert-file=/etc/kubernetes/pki/etcd/server.pem --key-file=/etc/kubernetes/pki/etcd/server-key.pem --client-cert-auth --trusted-ca-file=/etc/kubernetes/pki/etcd/ca.pem --peer-cert-file=/etc/kubernetes/pki/etcd/peer.pem --peer-key-file=/etc/kubernetes/pki/etcd/peer-key.pem --peer-client-cert-auth --peer-trusted-ca-file=/etc/kubernetes/pki/etcd/ca.pem --initial-cluster etcd0=http://192.168.0.50:2380,etcd1=http://192.168.0.51:2380,etcd2=http://192.168.0.52:2380 --initial-cluster-token my-etcd-token --initial-cluster-state new
```

If after VMs reboot, etcd cluster and service failed to restart, do below
on etcd0 first(will pending if etcd1 & etcd 2 not join) then etcd1 and etcd2.
```
systemctl stop etcd
rm -rf /var/lib/etcd/
systemctl daemon-reload
systemctl start etcd
```

# Set up master Load Balancer
You will need to ensure that the load balancer routes to just `master0` on port `6443`. This is because kubeadm will perform health checks using the load balancer IP. Since `master0` is set up individually first, the other masters will not have running apiservers, which will result in kubeadm hanging indefinitely.

If possible, first use a smart load balancing algorithm like least connections, second use health checks so unhealthy nodes can be removed from circulation. Most providers will provide these features.

First, [Nginx least connections load balancing method](https://www.nginx.com/resources/admin-guide/load-balancer/)</br>
The `least_conn` load balancing method might not work as expected without the zone directive, at least on small loads. This method of tcp and http load balancing passes a request to the server with the least number of active connections. Again, if the configuration of the group is not shared, each worker process uses its own counter for the number of connections. And if one worker process passes by a request to a server, the other worker process can also pass a request to the same server. However, you can increase the number of requests to reduce this effect. On high loads requests are distributed among worker processes evenly, and the least_conn load balancing method works as expected.

Second, [Nginx Passive Health Monitoring](https://www.nginx.com/resources/admin-guide/load-balancer/)</br>
When NGINX considers a server unavailable, it temporarily stops sending requests to that server until it is considered active again. The following parameters of the server directive configure the conditions to consider a server unavailable:

 * The `fail_timeout` parameter sets the time during which the specified number of failed attempts should happen and still consider the server unavailable. In other words, the server is unavailable for the interval set by fail_timeout.
 * The `max_fails` parameter sets the number of failed attempts that should happen during the specified time to still consider the server unavailable.

The **default values are 10 seconds and 1 attempt**. So if NGINX fails to send a request to some server or does not receive a response from this server at least once, it immediately considers the server unavailable for 10 seconds.

The following example shows how to set these parameters inside default ngixn config `/etc/nginx/sites-available/default`:
```
upstream backend {
    least_conn;

    server backend1.example.com;
    server backend2.example.com max_fails=3 fail_timeout=30s;
    server backend3.example.com max_fails=2;
}
```
Nginx also has Active Health Monitoring, you can go explore that topic further.

Nginx `/etc/nginx/nginx.conf` and `/etc/nginx/sites-available/default` structure explained:
1. https://linode.com/docs/web-servers/nginx/how-to-configure-nginx/
2. https://www.digitalocean.com/community/tutorials/how-to-configure-the-nginx-web-server-on-a-virtual-private-server

Nginx stores its configuration files within the "/etc/nginx" directory.

```
$ cd /etc/nginx
$ ls -F
conf.d/         koi-win           naxsi.rules   scgi_params       uwsgi_params
fastcgi_params  mime.types        nginx.conf    sites-available/  win-utf
koi-utf         naxsi_core.rules  proxy_params  sites-enabled/
```

The `sites-available` and `sites-enabled` directories are used to define configurations for your websites. Files are generally created in the `sites-available` directory, and then symbolically linked to the `sites-enabled` directory when they are ready to go live.

The `conf.d` directory can be used for site configuration as well. Every file within this directory ending with `.conf` is read into the configuration when Nginx is started, so make sure every file defines valid Nginx configuration syntax.

Before start, backup whole folder with a date.
```
cp -R /etc/nginx /etc/nginx.$(date "+%b_%d_%Y_%H.%M.%S")
```

## Exploring the nginx.conf File
```
root@master0:/etc/nginx# cat nginx.conf
user www-data;
worker_processes auto;
pid /run/nginx.pid;
include /etc/nginx/modules-enabled/*.conf;

events {
        worker_connections 768;
        # multi_accept on;
}

http{
...
```
**user**: Defines which Linux system user will own and run the nginx server. Most Debian-based distributions use www-data but this may be different in other distros. There are certain use cases that benefit from changing the user; for instance if you run two simultaneous web servers, or need another program’s user to have control over nginx.</br>
**worker_process**: Defines how many threads, or simultaneous instances, of nginx to run. You can learn more about this directive and the values of adjusting it here.</br>
**pid**: Defines where nginx will write its master process ID, or PID. The PID is used by the operating system to keep track of and send signals to the nginx process.</br>
This portion of the configuration file can also include things like error log locations using the "error_log" directive. `error_log  /var/log/nginx/error.log warn;`

The next section in our file is the `events` section. This is a special location that controls how Nginx will handle connections.

The following section is the http block which covers the universal directives for nginx as it handles HTTP web traffic. The first part of the HTTP block is shown below:
```
http {

        ##
        # Basic Settings
        ##

        sendfile on;
        tcp_nopush on;
        tcp_nodelay on;
        keepalive_timeout 65;
        types_hash_max_size 2048;
        # server_tokens off;

        # server_names_hash_bucket_size 64;
        # server_name_in_redirect off;

        include /etc/nginx/mime.types;
        default_type application/octet-stream;

        ##
        # SSL Settings
        ##

        ssl_protocols TLSv1 TLSv1.1 TLSv1.2; # Dropping SSLv3, ref: POODLE
        ssl_prefer_server_ciphers on;

        ##
        # Logging Settings
        ##

        access_log /var/log/nginx/access.log;
        error_log /var/log/nginx/error.log;

        ##
        # Gzip Settings
        ##

        gzip on;
        gzip_disable "msie6";

        # gzip_vary on;
        # gzip_proxied any;
        # gzip_comp_level 6;
        # gzip_buffers 16 8k;
        # gzip_http_version 1.1;
        # gzip_types text/plain text/css application/json application/javascript text/xml application/xml application/xml+rss text/javascript;

        ##
        # Virtual Host Configs
        ##

        include /etc/nginx/conf.d/*.conf;
        include /etc/nginx/sites-enabled/*;
}
```
**include**</br>
The `include` statement at the beginning of this section includes the file `mime.types` located at `/etc/nginx/mime.types`. What this means is that anything written in the file `mime.types` is interpreted as if it was written inside the http { } block. This lets you include a lengthy amount of information in the http { } block without having it clutter up the main configuration file. Try to avoid too many chained inclusions (i.e., including a file that itself includes a file, etc.) _Keep it to one or two levels of inclusion if possible, for readability purposes._</br>
You can always include all `.conf` files or simply all files in a certain directory with the directive:
```
include /etc/nginx/conf.d/*.conf;
include /etc/nginx/sites-enabled/*;
```
This tells us that the server and location blocks that define specific sites and url match locations will take place outside of this file.
This allows us to maintain a modular configuration arrangement where we can create new files when we would like to serve new sites. It allows us to group related content together,while hiding the details that do not change in most situations.

**gzip**
```
gzip on;
gzip_disable "msie6";
```
This tells Nginx to enable gzip to compress data that is sent to clients, but to disable gzip compression when the client is Internet Explorer version 6, because that browser does not understand gzip compression.
You should always put configuration details into the highest container to which they apply. This means that if you want parameter X to apply to every server block, then placing it within the http block will cause it to propagate to every server configuration.
If you have options that should have different values for some server blocks, you can specify them at a higher level and then override them within the server block. Nginx will take the lowest-level specification that applies to a setting.

## Exploring the Default Server Block
```
$ cd /etc/nginx/sites-available
$ sudo nano default
server {
    listen 80;
    listen [::]:80;

	root /usr/share/nginx/www;
	index index.html index.htm;

	server_name localhost;

	location / {
		try_files $uri $uri/ /index.html;
	}

	location /doc/ {
		alias /usr/share/doc/;
		autoindex on;
		allow 127.0.0.1;
		deny all;
	}
}
```
This block is placed into the `nginx.conf` file near the end of the http block, by using the `include` directive, as we discussed in the last section. The default file is very well-commented, but I've removed the comments to save space and demonstrate how simple the definition of a site can be.

The `listen` directive, which is located in the server block, tells nginx the hostname/IP and the TCP port where it should listen for HTTP connections. By default, nginx will listen for HTTP connections on port 80. Common examples for the `listen` directive.
```
listen 80 default_server;
listen [::]:80 default_server ipv6only=on;
listen     127.0.0.1:80;
listen     localhost:80;
```
The argument `default_server` means this virtual host will answer requests on port 80 that don’t specifically match another virtual host’s listen statement. The second statement listens over IPv6 and behaves in the same way.

The `root` directive defines the directory where the website's contents are located. This is the location where Nginx will start looking for files that are requested by the browser. The default website searches for its content in `/usr/share/nginx/www`.

The next line involves the `index` directive.
This configures the default pages served for the domain. If no page was requested, the server block will search for a file called `index.html` and return it. If it cannot find that file, it will try to serve a file called `index.htm`.

The `server_name` directive contains a list of domain names that will be served from this server block. You can include as many names as you would like, separated by spaces.

Location blocks are used to specify how certain resource requests are handled within a server.The line `location /` specifies that the directives within the brackets will apply to all resources requested by the client that do not match other location blocks.

The `try_files` directive is a very useful tool for defining a chain of attempts that should be made for resource requests.This means that when a request is made that is being served by that location block, Nginx will first try to serve the literal uri as a file. This is declared using the `$uri` variable, which will hold the resource being requested.

## Config ngxin on Master0
https://www.digitalocean.com/community/tutorials/how-to-set-up-nginx-load-balancing

https://www.digitalocean.com/community/tutorials/how-to-set-up-highly-available-haproxy-servers-with-keepalived-and-floating-ips-on-ubuntu-14-04

[Nginx runs on a seperate machine](
https://www.upcloud.com/support/how-to-set-up-load-balancing/)

[Nginx official round-robin, least-conn algo](
https://www.nginx.com/resources/admin-guide/load-balancer/)

[Ngxin Load Balancing example](
https://www.nginx.com/resources/wiki/start/topics/examples/loadbalanceexample/)

Install nginx `apt-get install nginx` in Master0, after installation, open url `localhost` in a browser, you should see the nginx page.

Config `/etc/nginx/nginx.conf` not `/etc/nginx/sites-available/default`
[cookeem example](https://github.com/cookeem/kubeadm-ha/blob/master/nginx-lb/nginx-lb.conf.tpl)
[klausenbusk post example](https://github.com/kubernetes/kubeadm/issues/546)
```
stream {
    upstream apiserver {
        least_conn;

        server 192.168.0.56:6443 weight=5 max_fails=3 fail_timeout=30s;
        # server K8SHA_IP2:6443 weight=1 max_fails=3 fail_timeout=15s;
        # server K8SHA_IP3:6443 weight=2 max_fails=2 fail_timeout=30s;
    }

    server {
        listen 192.168.0.100:16443;
        proxy_connect_timeout 1s;
        proxy_timeout 3s;
        proxy_pass apiserver;
    }
}
```
Add the master1 and master2 later for their IP.

# VirtualIP failover with Keepalived
http://dasunhegoda.com/nginx-reverse-proxying-load-balancing/1248/

```
apt-get install keepalived
nano /etc/keepalived/keepalived.conf

# Keepalived process identifier
lvs_id nginx_DH
}
# Script used to check if HAProxy is running
vrrp_script check_nginx {
script "killall -0 nginx"
interval 2
weight 2
}
# Virtual interface
# The priority specifies the order in which the assigned interface to take over in a failover
vrrp_instance VI_01 {
state SLAVE
interface eth0
virtual_router_id 51
priority 101
# The virtual ip address shared between the two loadbalancers
virtual_ipaddress {
FLOATING_IP
}
track_script {
check_nginx
}
}
```
`interface eth0` is `interface enp0s3`
`state SLAVE` is for Master1 and Master2, `state MASTER` is for Master0.
`FLOATING_IP` is 192.168.0.100 the virtual IP


Generate SSH keys for each of the master nodes by `ssh-keygen -t rsa -b 4096 -C "<email>"`. After doing this, each master will have an SSH key in `~/.ssh/id_rsa.pub` and an entry in etcd0’s `~/.ssh/authorized_keys` file.

Run the following to get cert from etcd0 to master:
```
mkdir -p /etc/kubernetes/pki/etcd
scp root@<etcd0-ip-address>:/etc/kubernetes/pki/etcd/ca.pem /etc/kubernetes/pki/etcd
scp root@<etcd0-ip-address>:/etc/kubernetes/pki/etcd/client.pem /etc/kubernetes/pki/etcd
scp root@<etcd0-ip-address>:/etc/kubernetes/pki/etcd/client-key.pem /etc/kubernetes/pki/etcd
```

Run `kubeadm init --config=config.yaml`
```
cat >config.yaml <<EOF
apiVersion: kubeadm.k8s.io/v1alpha1
kind: MasterConfiguration
api:
  advertiseAddress: <private-ip>
etcd:
  endpoints:
  - https://<etcd0-ip-address>:2379
  - https://<etcd1-ip-address>:2379
  - https://<etcd2-ip-address>:2379
  caFile: /etc/kubernetes/pki/etcd/ca.pem
  certFile: /etc/kubernetes/pki/etcd/client.pem
  keyFile: /etc/kubernetes/pki/etcd/client-key.pem
networking:
  podSubnet: <podCIDR>
apiServerCertSANs:
- <load-balancer-ip>
apiServerExtraArgs:
  endpoint-reconciler-type: lease
EOF
```

Ensure that the following placeholders are replaced:

`<private-ip>` with the private IPv4 of the master server.
`<etcd0-ip>`, `<etcd1-ip>` and `<etcd2-ip>` with the IP addresses of your three etcd nodes
`<podCIDR>` with your Pod CIDR (Flannel CNI `--pod-network-cidr=10.244.0.0/16`). Please read the CNI network section of the docs for more information. Some CNI providers do not require a value to be set.
`<load-balancer-ip>` from nginx
If you are using Kubernetes 1.9+, you can replace the `apiserver-count: 3` extra argument with `endpoint-reconciler-type=lease`.

Change endpoint to http in line with etcd cluster config.
```
---
apiVersion: kubeadm.k8s.io/v1alpha1
kind: MasterConfiguration
api:
  advertiseAddress: 192.168.0.56
etcd:
  endpoints:
  - http://192.168.0.50:2379
  - http://192.168.0.51:2379
  - http://192.168.0.52:2379
  caFile: /etc/kubernetes/pki/etcd/ca.pem
  certFile: /etc/kubernetes/pki/etcd/client.pem
  keyFile: /etc/kubernetes/pki/etcd/client-key.pem
networking:
  podSubnet: 10.244.0.0/16
apiServerCertSANs:
  - 192.168.0.100
apiServerExtraArgs:
  endpoint-reconciler-type: lease
```

After k8s initialised, run the common k8s recommandations
```
export KUBECONFIG=/etc/kubernetes/admin.conf

# install Flannel
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.9.1/Documentation/kube-flannel.yml

# 24h token
kubeadm join --token 5f151f.454be115e4281897 192.168.0.56:6443 --discovery-token-ca-cert-hash sha256:c39cf12c6a9e1956c74555e22dd40060530158d4a6ee5d3c48e0205818f8a0da

```

## Troubleshoot kubeadm init issues
* [Port-10250  is in use](https://github.com/kubernetes/kubeadm/issues/339)
```
[preflight] Some fatal errors occurred:
        [ERROR Port-10250]: Port 10250 is in use
        [ERROR Port-10251]: Port 10251 is in use
        [ERROR Port-10252]: Port 10252 is in use
```

Install net-tools package and check port, kill all pid and re-try
```
root@master0:apt-get install net-tools
root@master0:/etc/kubernetes/pki/etcd# netstat -a | grep 10250
tcp6       0      0 [::]:10250              [::]:*                  LISTEN
root@master0:/etc/kubernetes/pki/etcd# netstat -lnp | grep 1025
tcp        0      0 127.0.0.1:10251         0.0.0.0:*               LISTEN      1919/kube-scheduler
tcp        0      0 127.0.0.1:10252         0.0.0.0:*               LISTEN      1859/kube-controlle
tcp6       0      0 :::10255                :::*                    LISTEN      396/kubelet
tcp6       0      0 :::10250                :::*                    LISTEN      396/kubelet
root@master0:/etc/kubernetes/pki/etcd# kill 1919
root@master0:/etc/kubernetes/pki/etcd# kill 1859
root@master0:/etc/kubernetes/pki/etcd# kill 396
root@master0:/etc/kubernetes/pki/etcd# netstat -lnp | grep 1025
```

* etcd version "" empty. Such error means k8s can't talk to etcd cluster.
check kubeadm config.yaml their etcd endpoints https or http should be inline with etcd cluster.
```
Dec 13 12:52:01 ip-172-20-144-6 kubeadm[858]:         [ERROR ExternalEtcdVersion]: couldn't parse external etcd version "": Version string empty

```

* `systemctl status nginx` failed
[Suggestion](https://serverfault.com/questions/883073/nginx-emerg-stream-directive-is-not-allowed-here)
stream{} was configured inside `default` and `default` is included in `nginx.conf` blockk http{}, this nest is wrong.
stream{} and http{} needs to be at the same level in nginx, so delete http{} inside `nginx.conf` and add stream{}, keep default
as the orgin file.
```
root@master1:/etc/kubernetes/pki/etcd# systemctl status nginx
● nginx.service - A high performance web server and a reverse proxy server
   Loaded: loaded (/lib/systemd/system/nginx.service; disabled; vendor preset: enabled)
   Active: failed (Result: exit-code) since Sun 2018-02-11 11:28:23 EST; 15min ago
     Docs: man:nginx(8)
      CPU: 20ms

Feb 11 11:28:22 master1 systemd[1]: Starting A high performance web server and a reverse pFeb 11 11:28:23 master1 nginx[489]: nginx: [emerg] "stream" directive is not allowed here Feb 11 11:28:23 master1 nginx[489]: nginx: configuration file /etc/nginx/nginx.conf test fFeb 11 11:28:23 master1 systemd[1]: nginx.service: Control process exited, code=exited staFeb 11 11:28:23 master1 systemd[1]: Failed to start A high performance web server and a reFeb 11 11:28:23 master1 systemd[1]: nginx.service: Unit entered failed state.
Feb 11 11:28:23 master1 systemd[1]: nginx.service: Failed with result 'exit-code'.
```

# Run kubeadm init on other masters: master1 and master2
Before running kubeadm on the other masters, you need to first copy the K8s CA cert from master0.
```
# make sure master1 and master2 can access master0 via ssh
scp root@<master0-ip-address>:/etc/kubernetes/pki/* /etc/kubernetes/pki
rm apiserver.crt
```
The goal is to copy the contents of `/etc/kubernetes/pki/ca.crt` and `/etc/kubernetes/pki/ca.key` of master0 and create these files manually on master1 and master2.

Then change the IP address for `/etc/kubernetes/pki/etcd/config.yaml for master1 192.168.0.57 and master2 192.168.0.58
```
api:
  advertiseAddress: 192.168.0.56
```
Stop the nginx service on master1 and master2, keep them disabled
```
service nginx stop
systemctl disable nginx
```
Reconfig the `nginx.conf` inside master0 to activate the balancing on master1 and master2.

Repeat the `kubeadm init --config=config.yaml` for both master1 and master2, then install flannel as well.

# Add worksers in k8s cluster